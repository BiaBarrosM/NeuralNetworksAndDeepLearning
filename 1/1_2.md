---
mathjax: true
---

# Perceptrons

What is a neural network? To get started, I'll explain a type of artificial neuron called a perceptron. Perceptrons were [developed](https://books.google.ca/books/about/Principles_of_neurodynamics.html?id=7FhRAAAAMAAJ) in the 1950s and 1960s by the
scientist [Frank Rosenblatt](https://en.wikipedia.org/wiki/Frank_Rosenblatt), inspired by earlier [work](https://scholar.google.ca/scholar?cluster=4035975255085082870) by [Warren McCulloch](https://en.wikipedia.org/wiki/Warren_Sturgis_McCulloch) and [Walter Pitts](https://en.wikipedia.org/wiki/Walter_Pitts). Today, it's more common to use other models of artificial neurons - in
this book, and in much modern work on neural networks, the main neuron model used is one called the sigmoid neuron. We'll get to sigmoid neurons shortly. But to
understand why sigmoid neurons are defined the way they are, it's worth taking the time to first understand perceptrons.

So how do perceptrons work? A perceptron takes several binary inputs, x1,x2,…, and produces a single binary output:
<p align="center">
  <img src="http://neuralnetworksanddeeplearning.com/images/tikz0.png" width="350"/><br></i>
</p>

In the example shown the perceptron has three inputs, *x1,x2,x3*. In general it could have more or fewer inputs. Rosenblatt proposed a simple rule to compute the
output. He introduced *weights, w1,w2,…,* real numbers expressing the importance of the respective inputs to the output. The neuron's output, 0 or 1, is determined
by whether the weighted sum *∑jwjxj* is less than or greater than some *threshold value*. Just like the weights, the threshold is a real number which is a parameter
of the neuron. To put it in more precise algebraic terms:
