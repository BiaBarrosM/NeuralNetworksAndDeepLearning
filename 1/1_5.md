# Learning with gradient descent

Now that we have a design for our neural network, how can it learn to recognize digits? The first thing we'll need is a data set to learn from - a so-called 
training data set. We'll use the [MNIST data set](http://yann.lecun.com/exdb/mnist/), which contains tens of thousands of scanned images of handwritten digits, 
together with their correct classifications. MNIST's name comes from the fact that it is a modified subset of two data sets collected by 
[NIST](https://en.wikipedia.org/wiki/National_Institute_of_Standards_and_Technology), 
the United States' National Institute of Standards and Technology. Here's a few images from MNIST:

<p align="center">
  <img src="http://neuralnetworksanddeeplearning.com/images/digits_separate.png" width="400"/>
</p>

As you can see, these digits are, in fact, the same as those shown at the beginning of this chapter as a challenge to recognize. Of course, when testing our 
network we'll ask it to recognize images which aren't in the training set!
The MNIST data comes in two parts. The first part contains 60,000 images to be used as training data. These images are scanned handwriting samples from 250 
people, half of whom were US Census Bureau employees, and half of whom were high school students. The images are greyscale and 28 by 28 pixels in size. The second 
part of the MNIST data set is 10,000 images to be used as test data. Again, these are 28 by 28 greyscale images. We'll use the test data to evaluate how well our 
neural network has learned to recognize digits. To make this a good test of performance, the test data was taken from a different set of 250 people than the 
original training data (albeit still a group split between Census Bureau employees and high school students). This helps give us confidence that our system can 
recognize digits from people whose writing it didn't see during training.

We'll use the notation x to denote a training input. It'll be convenient to regard each training input x as a 28×28=784-dimensional vector. Each entry in the 
vector represents the grey value for a single pixel in the image. We'll denote the corresponding desired output by y=y(x), where y is a 10-dimensional vector. For 
example, if a particular training image, x, depicts a 6, then y(x)=(0,0,0,0,0,0,1,0,0,0)<sup>T</sup> is the desired output from the network. Note that T here is the 
transpose operation, turning a row vector into an ordinary (column) vector.

What we'd like is an algorithm which lets us find weights and biases so that the output from the network approximates y(x) for all training inputs x. To quantify 
how well we're achieving this goal we define a **cost function**<sup>[*](#fn1)</sup>


<p align="center">
  <img src="http://latex2png.com/pngs/d587b5ab5efc9bec5bc10c7c6352201b.png" width="400"/>
   <b><a name="6">(6)</a></b>
</p>
  
  
<a name="fn1">*</a>Sometimes referred to as a loss or objective function. We use the term cost function throughout this book, but you should note the
other terminology, since it's often used in research papers and other discussions of neural networks. </a>
  
Here, w denotes the collection of all weights in the network, b all the biases, n is the total number of training inputs, a is the vector of outputs from the 
network when x is input, and the sum is over all training inputs, x. Of course, the output a depends on x, w and b, but to keep the notation simple I haven't 
explicitly indicated this dependence. The notation ∥v∥ just denotes the usual length function for a vector v. We'll call C the quadratic cost function; it's also 
sometimes known as the mean squared error or just MSE. Inspecting the form of the quadratic cost function, we see that C(w,b) is non-negative, since every term in 
the sum is non-negative. Furthermore, the cost C(w,b) becomes small, i.e., C(w,b)≈0, precisely when y(x) is approximately equal to the output, a, for all training 
inputs, x. So our training algorithm has done a good job if it can find weights and biases so that C(w,b)≈0. By contrast, it's not doing so well when C(w,b) is 
large - that would mean that y(x) is not close to the output a for a large number of inputs. So the aim of our training algorithm will be to minimize the cost 
C(w,b) as a function of the weights and biases. In other words, we want to find a set of weights and biases which make the cost as small as possible. We'll do 
that using an algorithm known as _gradient descent_.

Why introduce the quadratic cost? After all, aren't we primarily interested in the number of images correctly classified by the network? Why not try to maximize 
that number directly, rather than minimizing a proxy measure like the quadratic cost? The problem with that is that the number of images correctly classified is 
not a smooth function of the weights and biases in the network. For the most part, making small changes to the weights and biases won't cause any change at all in 
the number of training images classified correctly. That makes it difficult to figure out how to change the weights and biases to get improved performance. If we 
instead use a smooth cost function like the quadratic cost it turns out to be easy to figure out how to make small changes in the weights and biases so as to get 
an improvement in the cost. That's why we focus first on minimizing the quadratic cost, and only after that will we examine the classification accuracy.

Even given that we want to use a smooth cost function, you may still wonder why we choose the quadratic function used in [Equation (6)](#6). Isn't this a rather 
ad hoc choice 
Perhaps if we chose a different cost function we'd get a totally different set of minimizing weights and biases? This is a valid concern, and later we'll revisit 
the cost function, and make some modifications. However, the quadratic cost function of [Equation (6)](#6) works perfectly well for understanding the basics of 
learning in neural networks, so we'll stick with it for now.

Recapping, our goal in training a neural network is to find weights and biases which minimize the quadratic cost function C(w,b). This is a well-posed problem, 
but it's got a lot of distracting structure as currently posed - the interpretation of w and b as weights and biases, the σ function lurking in the background, 
the choice of network architecture, MNIST, and so on. It turns out that we can understand a tremendous amount by ignoring most of that structure, and just 
concentrating on the minimization aspect. So for now we're going to forget all about the specific form of the cost function, the connection to neural networks, 
and so on. Instead, we're going to imagine that we've simply been given a function of many variables and we want to minimize that function. We're going to develop 
a technique called _gradient descent_ which can be used to solve such minimization problems. Then we'll come back to the specific function we want to minimize for neural networks.

Okay, let's suppose we're trying to minimize some function, C(v). This could be any real-valued function of many variables, v=v1,v2,…. Note that I've replaced the 
w and b notation by v to emphasize that this could be any function - we're not specifically thinking in the neural networks context any more. To minimize C(v) it 
helps to imagine C as a function of just two variables, which we'll call v1 and v2:

<p align="center">
  <img src="http://neuralnetworksanddeeplearning.com/images/valley.png" width="400"/>
</p>
 
What we'd like is to find where C achieves its global minimum. Now, of course, for the function plotted above, we can eyeball the graph and find the minimum. In 
that sense, I've perhaps shown slightly too simple a function! A general function, C, may be a complicated function of many variables, and it won't usually be 
possible to just eyeball the graph to find the minimum.

One way of attacking the problem is to use calculus to try to find the minimum analytically. We could compute derivatives and then try using them to find places 
where C is an extremum. With some luck that might work when C is a function of just one or a few variables. But it'll turn into a nightmare when we have many more 
variables. And for neural networks we'll often want far more variables - the biggest neural networks have cost functions which depend on billions of weights and 
biases in an extremely complicated way. Using calculus to minimize that just won't work!

(After asserting that we'll gain insight by imagining C as a function of just two variables, I've turned around twice in two paragraphs and said, "hey, but what 
if it's a function of many more than two variables?" Sorry about that. Please believe me when I say that it really does help to imagine C as a function of two 
variables. It just happens that sometimes that picture breaks down, and the last two paragraphs were dealing with such breakdowns. Good thinking about mathematics 
often involves juggling multiple intuitive pictures, learning when it's appropriate to use each picture, and when it's not.)

Okay, so calculus doesn't work. Fortunately, there is a beautiful analogy which suggests an algorithm which works pretty well. We start by thinking of our 
function as a kind of a valley. If you squint just a little at the plot above, that shouldn't be too hard. And we imagine a ball rolling down the slope of the 
valley. Our everyday experience tells us that the ball will eventually roll to the bottom of the valley. Perhaps we can use this idea as a way to find a minimum 
for the function? We'd randomly choose a starting point for an (imaginary) ball, and then simulate the motion of the ball as it rolled down to the bottom of the 
valley. We could do this simulation simply by computing derivatives (and perhaps some second derivatives) of C - those derivatives would tell us everything we 
need to know about the local "shape" of the valley, and therefore how our ball should roll.

Based on what I've just written, you might suppose that we'll be trying to write down Newton's equations of motion for the ball, considering the effects of 
friction and gravity, and so on. Actually, we're not going to take the ball-rolling analogy quite that seriously - we're devising an algorithm to minimize C, not 
developing an accurate simulation of the laws of physics! The ball's-eye view is meant to stimulate our imagination, not constrain our thinking. So rather than 
get into all the messy details of physics, let's simply ask ourselves: if we were declared God for a day, and could make up our own laws of physics, dictating to 
the ball how it should roll, what law or laws of motion could we pick that would make it so the ball always rolled to the bottom of the valley?

To make this question more precise, let's think about what happens when we move the ball a small amount Δv<sub>1</sub> in the v1 direction, and a small amount Δv<sub>2</sub> in the v2 
direction. Calculus tells us that C changes as follows:

<p align="center">
  <img src="http://latex2png.com/pngs/60afb2b669df457c6beec90c9a858234.png" width="400"/>
   <b><a name="7">(7)</a></b>
</p>

We're going to find a way of choosing Δv<sub>1</sub> and Δv<sub>2</sub> so as to make ΔC negative; i.e., we'll choose them so the ball is rolling down into the 
valley. To figure out how to make such a choice it helps to define Δv to be the vector of changes in v, Δv≡( Δv<sub>1</sub>,Δv<sub>2</sub>)<sup>T</sup>,where T is 
again the transpose operation, turning row vectors into column vectors. We'll also define the *gradient* of C to be the vector of partial derivatives, 
(<sup>∂C</sup>&frasl;<sub>∂v<sub>1</sub></sub>,<sup>∂C</sup>&frasl;<sub>∂v<sub>2</sub></sub>)<sup>T</sup>. We denote the gradient vector by ∇C, i.e.:

<p align="center">
  <img src="http://latex2png.com/pngs/3c55f1be0e469da44f083733a3d88dfd.png" width="400"/>
   <b><a name="8">(8)</a></b>
</p>

In a moment we'll rewrite the change ΔC in terms of Δv and the gradient, ∇C. Before getting to that, though, I want to clarify something that sometimes gets 
people hung up on the gradient. When meeting the ∇C notation for the first time, people sometimes wonder how they should think about the ∇ symbol. What, exactly, 
does ∇ mean? In fact, it's perfectly fine to think of ∇C as a single mathematical object - the vector defined above - which happens to be written using two 
symbols. In this point of view, ∇ is just a piece of notational flag-waving, telling you "hey, ∇C is a gradient vector". There are more advanced points of view 
where ∇ can be viewed as an independent mathematical entity in its own right (for example, as a differential operator), but we won't need such points of view.

With these definitions, the expression [(7)](#7) for ΔC can be rewritten as

<p align="center">
  <img src="http://latex2png.com/pngs/c608f25680080133556a862db0d6cab7.png" width="400"/>
   <b><a name="9">(9)</a></b>
</p>

This equation helps explain why ∇C is called the gradient vector: ∇C relates changes in v to changes in C, just as we'd expect something called a gradient to do. 
But what's really exciting about the equation is that it lets us see how to choose Δv so as to make ΔC negative. In particular, suppose we choose

<p align="center">
  <img src="http://latex2png.com/pngs/10fd26f3a27ff32f49ed5dc8760e92bc.png" width="400"/>
   <b><a name="10">(10)</a></b>
</p>

where η is a small, positive parameter (known as the learning rate). Then Equation [(9)](#9) tells us that ΔC≈−η∇C⋅∇C=−η∥∇C∥<sup>2</sup> . Because ∥∇C∥2≥0, this 
guarantees that ΔC≤0, i.e., C will always decrease, never increase, if we change v according to the prescription in [(10)](#10). (Within, of course, the limits of 
the approximation in Equation [(9)](#9)). This is exactly the property we wanted! And so we'll take Equation [(10)](#10) to define the "law of motion" for the 
ball in our gradient descent algorithm. That is, we'll use Equation [(10)](#10) to compute a value for Δv, then move the ball's position v by that amount:

<p align="center">
  <img src="http://latex2png.com/pngs/08ab3e5dd0df3b500dbe20b82e089d2a.png" width="400"/>
   <b><a name="11">(11)</a></b>
</p>

Then we'll use this update rule again, to make another move. If we keep doing this, over and over, we'll keep decreasing C until - we hope - we reach a global 
minimum.

Summing up, the way the gradient descent algorithm works is to repeatedly compute the gradient ∇C, and then to move in the *opposite* direction, "falling down" 
the slope of the valley. We can visualize it like this:

<p align="center">
  <img src="http://neuralnetworksanddeeplearning.com/images/valley_with_ball.png" width="400"/>
</p>

Notice that with this rule gradient descent doesn't reproduce real physical motion. In real life a ball has momentum, and that momentum may allow it to roll 
across the slope, or even (momentarily) roll uphill. It's only after the effects of friction set in that the ball is guaranteed to roll down into the valley. By 
contrast, our rule for choosing Δv just says "go down, right now". That's still a pretty good rule for finding the minimum!

To make gradient descent work correctly, we need to choose the learning rate η to be small enough that Equation [(9)](#9) is a good approximation. If we don't, we 
might end up with ΔC>0, which obviously would not be good! At the same time, we don't want η to be too small, since that will make the changes Δv tiny, and thus 
the gradient descent algorithm will work very slowly. In practical implementations, η is often varied so that Equation [(9)](#9) remains a good approximation, but 
the algorithm isn't too slow. We'll see later how this works.

I've explained gradient descent when C is a function of just two variables. But, in fact, everything works just as well even when C is a function of many more 
variables. Suppose in particular that C is a function of *m* variables, v1,…,vm. Then the change ΔC in C produced by a small change Δv=(Δv<sub>1</sub>,…,Δv<sub>m</sub>)<sup>T</sup> is

<p align="center">
  <img src="http://latex2png.com/pngs/5dc196bbe50035aed3eba38becfc5b8a.png" width="400"/>
   <b><a name="12">(12)</a></b>
</p>

where the gradient ∇C is the vector

<p align="center">
  <img src="http://latex2png.com/pngs/10e1f7b7c16d6b6e43189de6fc670199.png" width="400"/>
   <b><a name="13">(13)</a></b>
</p>

Just as for the two variable case, we can choose

<p align="center">
  <img src="http://latex2png.com/pngs/10fd26f3a27ff32f49ed5dc8760e92bc.png" width="400"/>
   <b><a name="14">(14)</a></b>
</p>

and we're guaranteed that our (approximate) expression [(12)](#12) for ΔC will be negative. This gives us a way of following the gradient to a minimum, even when 
C is a function of many variables, by repeatedly applying the update rule

<p align="center">
  <img src="http://latex2png.com/pngs/780a0df23b9263e865a53ca07b811d6b.png" width="400"/>
   <b><a name="15">(15)</a></b>
</p>

You can think of this update rule as defining the gradient descent algorithm. It gives us a way of repeatedly changing the position v in order to find a minimum 
of the function C. The rule doesn't always work - several things can go wrong and prevent gradient descent from finding the global minimum of C, a point we'll 
return to explore in later chapters. But, in practice gradient descent often works extremely well, and in neural networks we'll find that it's a powerful way of 
minimizing the cost function, and so helping the net learn.

Indeed, there's even a sense in which gradient descent is the optimal strategy for searching for a minimum. Let's suppose that we're trying to make a move Δv in 
position so as to decrease C as much as possible. This is equivalent to minimizing ΔC≈∇C⋅Δv. We'll constrain the size of the move so that ∥Δv∥=ϵ for some small 
fixed ϵ>0. In other words, we want a move that is a small step of a fixed size, and we're trying to find the movement direction which decreases C as much as 
possible. It can be proved that the choice of Δv which minimizes ∇C⋅Δv is Δv=−η∇C, where η=ϵ/∥∇C∥ is determined by the size constraint ∥Δv∥=ϵ. So gradient descent 
can be viewed as a way of taking small steps in the direction which does the most to immediately decrease C.

## Exercises
