# Backpropagation: the big picture

As I've explained it, backpropagation presents two mysteries. First, what's the algorithm really doing? We've developed a picture of the error being backpropagated from the output. But can we go any deeper, and build up more intuition about what is going on when we do all these matrix and vector multiplications? The second mystery is how someone could ever have discovered backpropagation in the first place? It's one thing to follow the steps in an algorithm, or even to follow the proof that the algorithm works. But that doesn't mean you understand the problem so well that you could have discovered the algorithm in the first place. Is there a plausible line of reasoning that could have led you to discover the backpropagation algorithm? In this section I'll address both these mysteries.

To improve our intuition about what the algorithm is doing, let's imagine that we've made a small change Δw<sup>l</sup><sub>jk</sub> to some weight in the network, w<sup>l</sup><sub>jk</sub>:
<p align="center">
  <img src="https://user-images.githubusercontent.com/77112891/146607986-3bc88c2a-b6ad-4041-8f4b-1b94af9f660e.png" width="400"/> 
</p>

That change in weight will cause a change in the output activation from the corresponding neuron:
<p align="center">
  <img src="https://user-images.githubusercontent.com/77112891/146608171-30ba0741-f355-4540-9d89-29031242144c.png" width="400"/> 
</p>

That, in turn, will cause a change in all the activations in the next layer:
<p align="center">
  <img src="https://user-images.githubusercontent.com/77112891/146608222-26cb06bf-2310-4862-a015-d1a705417847.png" width="400"/> 
</p>

Those changes will in turn cause changes in the next layer, and then the next, and so on all the way through to causing a change in the final layer, and then in the cost function:
<p align="center">
  <img src="https://user-images.githubusercontent.com/77112891/146608255-79466bea-ef91-41f9-b378-7f3375c0b6db.png" width="400"/> 
</p>

The change ΔC in the cost is related to the change Δw<sup>l</sup><sub>jk</sub> in the weight by the equation
<p align="center">
  <img src="http://latex2png.com/pngs/22ebc7561958b3035f91522d34a22b40.png" width="400"/>
   <b><a name="47">(47)</a></b>
</p>

This suggests that a possible approach to computing ∂C/w<sup>l</sup><sub>jk</sub>  is to carefully track how a small change in wljk propagates to cause a small change in C. If we can do that, being careful to express everything along the way in terms of easily computable quantities, then we should be able to compute ∂C/∂w<sup>l</sup><sub>jk</sub>.

Let's try to carry this out. The change Δw<sup>l</sup><sub>jk</sub> causes a small change Δa<sup>l</sup><sub>j</sub> in the activation of the j<sup>th</sup> neuron in the l<sup>th</sup> layer. This change is given by
<p align="center">
  <img src="http://latex2png.com/pngs/5da106b2c6d2444a5505d5f33a270be1.png" width="400"/>
   <b><a name="48">(48)</a></b>
</p>

The change in activation Δa<sup>l</sup><sub>j</sub> will cause changes in all the activations in the next layer, i.e., the (l+1)<sup>th</sup> layer. We'll concentrate on the way just a single one of those activations is affected, say a<sup>l+1</sup><sub>q</sub>,
<p align="center">
  <img src="https://user-images.githubusercontent.com/77112891/146610557-270c8317-58fa-4876-affc-ec0973695cdb.png" width="400"/>
</p>

In fact, it'll cause the following change:
<p align="center">
  <img src="http://latex2png.com/pngs/cbb34293f3e22cf2a788c1aba3868a19.png" width="400"/>
   <b><a name="49">(49)</a></b>
</p>

Substituting in the expression from [Equation (48)](#48), we get:
<p align="center">
  <img src="http://latex2png.com/pngs/2c3cbfd6672748a74101e29618ff2c78.png" width="400"/>
   <b><a name="50">(50)</a></b>
</p>

Of course, the change Δa<sup>l+1</sup><sub>q</sub> will, in turn, cause changes in the activations in the next layer. In fact, we can imagine a path all the way through the network from w<sup>l</sup><sub>jk</sub> to C, with each change in activation causing a change in the next activation, and, finally, a change in the cost at the output. If the path goes through activations a<sup>l</sup><sub>j</sub>,a<sup>l+1</sup><sub>q</sub>,…,a<sup>L-1</sup><sub>n</sub>, a<sup>L</sup><sub>m</sub> then the resulting expression is
<p align="center">
  <img src="http://latex2png.com/pngs/0aa64d3dab701cc31b59b0a643152e71.png" width="400"/>
   <b><a name="51">(51)</a></b>
</p>

that is, we've picked up a ∂a/∂a type term for each additional neuron we've passed through, as well as the ∂C/∂a<sup>L</sup><sub>m</sub> term at the end. This represents the change in C due to changes in the activations along this particular path through the network. Of course, there's many paths by which a change in w<sup>l</sup><sub>jk</sub> can propagate to affect the cost, and we've been considering just a single path. To compute the total change in C it is plausible that we should sum over all the possible paths between the weight and the final cost, i.e.,
<p align="center">
  <img src="http://latex2png.com/pngs/7fccdb54ff58c8b88b40b7aeeb0a8e00.png" width="400"/>
   <b><a name="52">(52)</a></b>
</p>

where we've summed over all possible choices for the intermediate neurons along the path. Comparing with [(47)](#47) we see that
<p align="center">
  <img src="http://latex2png.com/pngs/a7c85581a5e79f01f9f7942eef94a9b8.png" width="400"/>
   <b><a name="53">(53)</a></b>
</p>

Now, [(Equation (53))](#53) looks complicated. However, it has a nice intuitive interpretation. We're computing the rate of change of C with respect to a weight in the network. What the equation tells us is that every edge between two neurons in the network is associated with a rate factor which is just the partial derivative of one neuron's activation with respect to the other neuron's activation. The edge from the first weight to the first neuron has a rate factor ∂a<sup>l</sup><sub>j</sub>/∂wa<sup>l</sup><sub>jk</sub>. The rate factor for a path is just the product of the rate factors along the path. And the total rate of change ∂C/∂wa<sup>l</sup><sub>jk</sub> is just the sum of the rate factors of all paths from the initial weight to the final cost. This procedure is illustrated here, for a single path:
<p align="center">
  <img src="https://user-images.githubusercontent.com/77112891/146611648-d4ef2949-5f6f-4fef-9ff1-95fca65dfdcd.png" width="400"/>
   <b><a name="53">(53)</a></b>
</p>

What I've been providing up to now is a heuristic argument, a way of thinking about what's going on when you perturb a weight in a network. Let me sketch out a line of thinking you could use to further develop this argument. First, you could derive explicit expressions for all the individual partial derivatives in [(Equation (53))](#53). That's easy to do with a bit of calculus. Having done that, you could then try to figure out how to write all the sums over indices as matrix multiplications. This turns out to be tedious, and requires some persistence, but not extraordinary insight. After doing all this, and then simplifying as much as possible, what you discover is that you end up with exactly the backpropagation algorithm! And so you can think of the backpropagation algorithm as providing a way of computing the sum over the rate factor for all these paths. Or, to put it slightly differently, the backpropagation algorithm is a clever way of keeping track of small perturbations to the weights (and biases) as they propagate through the network, reach the output, and then affect the cost.

Now, I'm not going to work through all this here. It's messy and requires considerable care to work through all the details. If you're up for a challenge, you may enjoy attempting it. And even if not, I hope this line of thinking gives you some insight into what backpropagation is accomplishing.

What about the other mystery - how backpropagation could have been discovered in the first place? In fact, if you follow the approach I just sketched you will discover a proof of backpropagation. Unfortunately, the proof is quite a bit longer and more complicated than the one I described earlier in this chapter. So how was that short (but more mysterious) proof discovered? What you find when you write out all the details of the long proof is that, after the fact, there are several obvious simplifications staring you in the face. You make those simplifications, get a shorter proof, and write that out. And then several more obvious simplifications jump out at you. So you repeat again. The result after a few iterations is the proof we saw earlier* - short, but somewhat obscure, because all the signposts to its construction have been removed! I am, of course, asking you to trust me on this, but there really is no great mystery to the origin of the earlier proof. It's just a lot of hard work simplifying the proof I've sketched in this section.
