# Proof of the four fundamental equations (optional)

We'll now prove the four fundamental equations (BP1)-(BP4). All four are consequences of the chain rule from multivariable calculus. If you're comfortable with the chain rule, then I strongly encourage you to attempt the derivation yourself before reading on.

Let's begin with Equation (BP1), which gives an expression for the output error, δL. To prove this equation, recall that by definition:
<p align="center">
  <img src="https://user-images.githubusercontent.com/48807586/137405204-7a6ef55c-403e-4313-ad3b-e9c8283e1eab.png" width="100"/> (36)<br>
</p>

Applying the chain rule, we can re-express the partial derivative above in terms of partial derivatives with respect to the output activations,
<p align="center">
  <img src="https://user-images.githubusercontent.com/48807586/137405330-a7f5ed26-fbc7-4752-8fa5-bd65e6820098.png" width="150"/> (37)<br>
</p>

where the sum is over all neurons k in the output layer. Of course, the output activation <img src="https://user-images.githubusercontent.com/48807586/137405406-e7799eba-38a2-465a-98e1-2739f960c4b6.png" width="20"/> of the kth neuron depends only on the weighted input <img src="https://user-images.githubusercontent.com/48807586/137405553-23b84bd2-0ab1-4c04-bc9f-201d27642238.png" width="20"/> for the jth neuron when k=j. And so <img src="https://user-images.githubusercontent.com/48807586/137405802-bfae2da0-27f7-47d3-976a-a7c547a5b444.png" width="55"/> vanishes when k≠j. As a result we can simplify the previous equation to

<p align="center">
  <img src="https://user-images.githubusercontent.com/48807586/137405760-0e52a6fa-479b-4460-8d58-33d17c8e142b.png" width="150"/> (38)<br>
</p>

Recalling that <img src="https://user-images.githubusercontent.com/48807586/137405929-26e84cbf-3915-4f41-a1e1-899cf9811c54.png" width="60"/> the second term on the right can be written as  <img src="https://user-images.githubusercontent.com/48807586/137405971-b408e0a0-2ae3-4f4e-aa03-91eb7c8ca9c0.png" width="30"/>, and the equation becomes

<p align="center">
  <img src="https://user-images.githubusercontent.com/48807586/137406097-06b36102-2690-439c-8420-73d77e09b122.png" width="150"/> (39)<br>
</p>

which is just (BP1), in component form. Next, we'll prove (BP2), which gives an equation for the error δ(l) in terms of the error in the next layer, δ(l+1). To do this, we want to rewrite <img src="https://user-images.githubusercontent.com/48807586/137406183-5fed3f75-84d0-48e4-9baf-5e1a95eeb082.png" width="80"/> in terms of <img src="https://user-images.githubusercontent.com/48807586/137406183-5fed3f75-84d0-48e4-9baf-5e1a95eeb082.png" width="80"/>. We can do this using the chain rule,

<p align="center">
  <img src="https://user-images.githubusercontent.com/48807586/137416132-e35d84d8-d5c5-4abe-b44c-94fd13cfc916.png" width="150"/> (40-42)<br>
</p>

where in the last line we have interchanged the two terms on the right-hand side, and substituted the definition of δ(l+1)k. To evaluate the first term on the last line, note that

<p align="center">
  <img src="https://user-images.githubusercontent.com/48807586/137417459-ff8bf5c5-af26-4cd1-9f64-71bf12845533.png" width="350"/> (43)<br>
</p>

Differentiating, we obtain

<p align="center">
  <img src="https://user-images.githubusercontent.com/48807586/137417910-b2fc6ee5-01b7-41ed-974c-bd66fa5d61d5.png" width="150"/> (44)<br>
</p>

Substituting back into (42) we obtain

<p align="center">
  <img src="https://user-images.githubusercontent.com/48807586/137417826-f80731c3-0e66-498d-a407-f591d0cb2b45.png" width="150"/> (45)<br>
</p>

This is just (BP2) written in component form.

The final two equations we want to prove are (BP3) and (BP4). These also follow from the chain rule, in a manner similar to the proofs of the two equations above. I leave them to you as an exercise.

## Exercise
- Prove Equations (BP3) and (BP4).

That completes the **proof of the four fundamental equations** of backpropagation. The proof may seem complicated. But it's really just the outcome of carefully applying the chain rule. A little less succinctly, we can think of backpropagation as a way of computing the gradient of the cost function by systematically applying the chain rule from multi-variable calculus. That's all there really is to backpropagation - the rest is details.
