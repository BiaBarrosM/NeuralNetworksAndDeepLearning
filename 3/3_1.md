# The cross-entropy cost function

# Softmax
In this chapter we'll mostly use the cross-entropy cost to address the problem of learning slowdown. However, I want to briefly describe another approach to the problem, based on what are called *softmax* layers of neurons. We're not actually going to use softmax layers in the remainder of the chapter, so if you're in a great hurry, you can skip to the next section. However, softmax is still worth understanding, in part because it's intrinsically interesting, and in part because we'll use softmax layers in [Chapter 6](http://neuralnetworksanddeeplearning.com/chap6.html), in our discussion of deep neural networks.  
  
The idea of softmax is to define a new type of output layer for our neural networks. It begins in the same way as with a sigmoid layer, by forming the weighted inputs* z<sub>j</sub><sup>L</sup> = ∑<sub>k</sub>w<sup>L</sup><sub>jk</sub>a<sup>L−1</sup><sub>k</sub>+b<sub>j</sub><sup>L</sup>. However, we don't apply the sigmoid function to get the output. Instead, in a softmax layer we apply the so-called softmax function to the z<sub>j</sub><sup>L</sup>. According to this function, the activation a<sub>j</sub><sup>L</sup> of the <sub>j</sub>th output neuron is

>> \* In describing the softmax we'll make frequent use of notation introduced in the last chapter. You may wish to revisit that chapter if you need to refresh your memory about the meaning of the notation.

<p align="center">
  <img src="https://user-images.githubusercontent.com/57269172/144334766-7194ae36-0c3f-4c58-9cfa-947147001006.png" width="250"/>  
<b>(78)</b>
</p>

where in the denominator we sum over all the output neurons.  

If you're not familiar with the softmax function, Equation `(78)` may look pretty opaque. It's certainly not obvious why we'd want to use this function. And it's also not obvious that this will help us address the learning slowdown problem. To better understand Equation `(78)`, suppose we have a network with four output neurons, and four corresponding weighted inputs, which we'll denote z<sup>L</sup><sub>1</sub>, z<sup>L</sup><sub>2</sub>, z<sup>L</sup><sub>3</sub>, and z<sup>L</sup><sub>4</sub>. Shown below are adjustable sliders showing possible values for the weighted inputs, and a graph of the corresponding output activations. A good place to start exploration is by using the bottom slider to increase z<sup>L</sup><sub>j</sub>:

-- slider -- 

As you increase z<sup>L</sup><sub>j</sub>, you'll see an increase in the corresponding output activation, a<sup>L</sup><sub>j</sub>, and a decrease in the other output activations. Similarly, if you decrease z<sup>L</sup><sub>j</sub> then a<sup>L</sup><sub>j</sub> will decrease, and all the other output activations will increase. In fact, if you look closely, you'll see that in both cases the total change in the other activations exactly compensates for the change in a<sup>L</sup><sub>j</sub>. The reason is that the output activations are guaranteed to always sum up to 1, as we can prove using Equation `(78)` and a little algebra:

<p align="center">
  <img src="https://user-images.githubusercontent.com/57269172/144335372-23102c0d-1dc7-43c4-b4ae-b4a8efd52e8b.png" width="200"/>  
<b>(79)</b>
</p>

As a result, if a<sup>L</sup><sub>j</sub> increases, then the other output activations must decrease by the same total amount, to ensure the sum over all activations remains 1. And, of course, similar statements hold for all the other activations.  

Equation `(78)` also implies that the output activations are all positive, since the exponential function is positive. Combining this with the observation in the last paragraph, we see that the output from the softmax layer is a set of positive numbers which sum up to 1. In other words, the output from the softmax layer can be thought of as a probability distribution.  

The fact that a softmax layer outputs a probability distribution is rather pleasing. In many problems it's convenient to be able to interpret the output activation aLj as the network's estimate of the probability that the correct output is j. So, for instance, in the MNIST classification problem, we can interpret aLj as the network's estimated probability that the correct digit classification is j.  

By contrast, if the output layer was a sigmoid layer, then we certainly couldn't assume that the activations formed a probability distribution. I won't explicitly prove it, but it should be plausible that the activations from a sigmoid layer won't in general form a probability distribution. And so with a sigmoid output layer we don't have such a simple interpretation of the output activations.  

# Exercise 

* Construct an example showing explicitly that in a network with a sigmoid output layer, the output activations a<sup>L</sup><sub>j</sub> won't always sum to 1.

We're starting to build up some feel for the softmax function and the way softmax layers behave. Just to review where we're at: the exponentials in Equation `(78)` ensure that all the output activations are positive. And the sum in the denominator of Equation `(78)` ensures that the softmax outputs sum to 1. So that particular form no longer appears so mysterious: rather, it is a natural way to ensure that the output activations form a probability distribution. You can think of softmax as a way of rescaling the z<sup>L</sup><sub>j</sub>, and then squishing them together to form a probability distribution.  

# Exercises 

* **Monotonicity of softmax** Show that ∂a<sup>L</sup><sub>j</sub>/∂z<sup>L</sup><sub>j</sub> is positive if j=k and negative if j≠k. As a consequence, increasing z<sup>L</sup><sub>j</sub> is guaranteed to increase the corresponding output activation, a<sup>L</sup><sub>j</sub>, and will decrease all the other output activations. We already saw this empirically with the sliders, but this is a rigorous proof.  

* **Non-locality of softmax** A nice thing about sigmoid layers is that the output a<sup>L</sup><sub>j</sub> is a function of the corresponding weighted input, a<sup>L</sup><sub>j</sub>=σ(z<sup>L</sup><sub>j</sub>). Explain why this is not the case for a softmax layer: any particular output activation a<sup>L</sup><sub>j</sub> depends on all the weighted inputs.

# Problem

* **Inverting the softmax layer** Suppose we have a neural network with a softmax output layer, and the activations aLj are known. Show that the corresponding weighted inputs have the form z<sup>L</sup><sub>j</sub>=lna<sup>L</sup><sub>j</sub>+C, for some constant C that is independent of j.  

**The learning slowdown problem:** We've now built up considerable familiarity with softmax layers of neurons. But we haven't yet seen how a softmax layer lets us address the learning slowdown problem. To understand that, let's define the log-likelihood cost function. We'll use x to denote a training input to the network, and y to denote the corresponding desired output. Then the log-likelihood cost associated to this training input is

<p align="center">
  <img src="https://user-images.githubusercontent.com/57269172/144335985-ddb242f3-0622-4383-a055-131c524326f6.png" width="175"/>  
<b>(80)</b>
</p>

So, for instance, if we're training with MNIST images, and input an image of a 7, then the log-likelihood cost is −lna<sup>L</sup><sub>j</sub>. To see that this makes intuitive sense, consider the case when the network is doing a good job, that is, it is confident the input is a 7. In that case it will estimate a value for the corresponding probability a<sup>L</sup><sub>7</sub> which is close to 1, and so the cost −lnaL7 will be small. By contrast, when the network isn't doing such a good job, the probability a<sup>L</sup><sub>7</sub> will be smaller, and the cost −lna<sup>L</sup><sub>7</sub> will be larger. So the log-likelihood cost behaves as we'd expect a cost function to behave.

What about the learning slowdown problem? To analyze that, recall that the key to the learning slowdown is the behaviour of the quantities ∂C/∂w<sup>L</sup><sub>jk</sub> and ∂C/∂b<sup>L</sup><sub>j</sub>. I won't go through the derivation explicitly - I'll ask you to do in the problems, below - but with a little algebra you can show that*

>> \* Note that I'm abusing notation here, using y in a slightly different way to last paragraph. In the last paragraph we used y to denote the desired output from the network - e.g., output a "7" if an image of a 7 was input. But in the equations which follow I'm using y to denote the vector of output activations which corresponds to 7, that is, a vector which is all 0s, except for a 1 in the 7th location.

<p align="center">
  <img src="https://user-images.githubusercontent.com/57269172/144336146-112ed89b-7623-4d53-b577-113751316527.png" width="175"/>  
<b>(81)</b>
</p>

<p align="center">
  <img src="https://user-images.githubusercontent.com/57269172/144336227-9606131b-8760-4186-9c4d-f83bee43d1a8.png" width="250"/>  
<b>(82)</b>
</p>

These equations are the same as the analogous expressions obtained in our earlier analysis of the cross-entropy. Compare, for example, Equation `(82)` to Equation `(67)`. 's the same equation, albeit in the latter I've averaged over training instances. And, just as in the earlier analysis, these expressions ensure that we will not encounter a learning slowdown. In fact, it's useful to think of a softmax output layer with log-likelihood cost as being quite similar to a sigmoid output layer with cross-entropy cost.

Given this similarity, should you use a sigmoid output layer and cross-entropy, or a softmax output layer and log-likelihood? In fact, in many situations both approaches work well. Through the remainder of this chapter we'll use a sigmoid output layer, with the cross-entropy cost. Later, in [Chapter 6](http://neuralnetworksanddeeplearning.com/chap6.html), we'll sometimes use a softmax output layer, with log-likelihood cost. The reason for the switch is to make some of our later networks more similar to networks found in certain influential academic papers. As a more general point of principle, softmax plus log-likelihood is worth using whenever you want to interpret the output activations as probabilities. That's not always a concern, but can be useful with classification problems (like MNIST) involving disjoint classes.

# Problems 

* Derive Equations `(81)` and `(82)`.  

* **Where does the "softmax" name come from?** Suppose we change the softmax function so the output activations are given by

<p align="center">
  <img src="https://user-images.githubusercontent.com/57269172/144336493-4498924e-4254-46c2-b4ab-bb62ce26a4ca.png" width="175"/>  
<b>(83)</b>
</p>

* where c is a positive constant. Note that c=1 corresponds to the standard softmax function. But if we use a different value of c we get a different function, which is nonetheless qualitatively rather similar to the softmax. In particular, show that the output activations form a probability distribution, just as for the usual softmax. Suppose we allow c to become large, i.e., c→∞. What is the limiting value for the output activations aLj? After solving this problem it should be clear to you why we think of the c=1 function as a "softened" version of the maximum function. This is the origin of the term "softmax".  

* **Backpropagation with softmax and the log-likelihood cost** In the last chapter we derived the backpropagation algorithm for a network containing sigmoid layers. To apply the algorithm to a network with a softmax layer we need to figure out an expression for the error δLj≡∂C/∂zLj in the final layer. Show that a suitable expression is:

<p align="center">
  <img src="https://user-images.githubusercontent.com/57269172/144336588-49e63350-5a29-4139-960c-5bb47ce138df.png" width="175"/>  
<b>(84)</b>
</p>  

* Using this expression we can apply the backpropagation algorithm to a network using a softmax output layer and the log-likelihood cost.


=======
## Exercises 

* One gotcha with the cross-entropy is that it can be difficult at first to remember the respective roles of the ys and the as. It's easy to get confused about whether the right form is -[ylna + (1-y) ln(1-a)] or -[a lny + (1-a) ln(1-y)]. What happens to the second of these expressions when y=0 or 1? Does this problem afflict the first expression? Why or why not?
* In the single-neuron discussion at the start of this section, I argued that the cross-entropy is small if σ(z)≈y for all training inputs. The argument relied on y being equal to either 0 or 1. This is usually true in classification problems, but for other problems (e.g., regression problems) y can sometimes take values intermediate between 0 and 1. Show that the cross-entropy is still minimized when σ(z)=y for all training inputs. When this is the case the cross-entropy has the value:

<p align="center">
  <img src="http://latex2png.com/pngs/458e94840b009ae8a46d5849344ffb73.png" width="400"/><br>
  <b><a name="64">(64)</a></b>
</p>

The quantity −[ylny+(1−y)ln(1−y)] is sometimes known as the [binary entropy](http://en.wikipedia.org/wiki/Binary_entropy_function).

## Problems 

* **Many-layer multi-neuron networks** In the notation introduced in the [last chapter](http://neuralnetworksanddeeplearning.com/chap2.html), show that for the quadratic cost the partial derivative with respect to weights in the output layer is

<p align="center">
  <img src="http://latex2png.com/pngs/d79a7f53bc253175f7a189bede47240c.png" width="400"/><br>
  <b><a name="65">(65)</a></b>
</p>

The term σ′(z<sub>j</sub><sup>L</sup>) causes a learning slowdown whenever an output neuron saturates on the wrong value. Show that for the cross-entropy cost the output error δ<sup>L</sup> for a single training example x is given by


<p align="center">
  <img src="http://latex2png.com/pngs/e3aecd9e0cfe75f1fc8372b93f4ed4a1.png" width="400"/><br>
  <b><a name="66">(66)</a></b>
</p>

Use this expression to show that the partial derivative with respect to the weights in the output layer is given by

<p align="center">
  <img src="http://latex2png.com/pngs/3367f413a20b9fbcbd6dda6e61ff6876.png" width="400"/><br>
  <b><a name="67">(67)</a></b>
</p>

The σ′(z<sub>j</sub><sup>L</sup>) term has vanished, and so the cross-entropy avoids the problem of learning slowdown, not just when used with a single neuron, as we saw earlier, but also in many-layer multi-neuron networks. A simple variation on this analysis holds also for the biases. If this is not obvious to you, then you should work through that analysis as well.

* **Using the quadratic cost when we have linear neurons in the output layer** Suppose that we have a many-layer multi-neuron network. Suppose all the neurons in the final layer are linear neurons, meaning that the sigmoid activation function is not applied, and the outputs are simply a<sub>j</sub><sup>j</sup>=z<sub>j</sub><sup>L</sup>. Show that if we use the quadratic cost function then the output error δL for a single training example x is given by

<p align="center">
  <img src="http://latex2png.com/pngs/e3aecd9e0cfe75f1fc8372b93f4ed4a1.png" width="400"/><br>
  <b><a name="68">(68)</a></b>
</p>

Similarly to the previous problem, use this expression to show that the partial derivatives with respect to the weights and biases in the output layer are given by

<p align="center">
  <img src="http://latex2png.com/pngs/b60c991861408e5ddd7449ea62f042b1.png" width="400"/><br>
  <b><a name="69">(69)</a></b>
</p>

<p align="center">
  <img src="http://latex2png.com/pngs/a86ea75468101e228ab8d5ba2f42500f.png" width="400"/><br>
  <b><a name="70">(70)</a></b>
</p>

This shows that if the output neurons are linear neurons then the quadratic cost will not give rise to any problems with a learning slowdown. In this case the quadratic cost is, in fact, an appropriate cost function to use.

## Using the cross-entropy to classify MNIST digits

The cross-entropy is easy to implement as part of a program which learns using gradient descent and backpropagation. We'll do that [later in the chapter](http://neuralnetworksanddeeplearning.com/chap3.html#handwriting_recognition_revisited_the_code), developing an improved version of our [earlier program](http://neuralnetworksanddeeplearning.com/chap1.html#implementing_our_network_to_classify_digits) for classifying the MNIST handwritten digits, `network.py`. The new program is called `network2.py`, and incorporates not just the cross-entropy, but also several other techniques developed in this chapter. For now, let's look at how well our new program classifies MNIST digits. As was the case in Chapter 1, we'll use a network with 30 hidden neurons, and we'll use a mini-batch size of 10. We set the learning rate to η=0.5 and we train for 30 epochs. The interface to `network2.py` is slightly different than `network.py`, but it should still be clear what is going on. You can, by the way, get documentation about `network2.py`'s interface by using commands such as `help(network2.Network.SGD)` in a Python shell.

