# The cross-entropy cost function

## Exercises 

* One gotcha with the cross-entropy is that it can be difficult at first to remember the respective roles of the ys and the as. It's easy to get confused about whether the right form is -[ylna + (1-y) ln(1-a)] or -[a lny + (1-a) ln(1-y)]. What happens to the second of these expressions when y=0 or 1? Does this problem afflict the first expression? Why or why not?
* In the single-neuron discussion at the start of this section, I argued that the cross-entropy is small if σ(z)≈y for all training inputs. The argument relied on y being equal to either 0 or 1. This is usually true in classification problems, but for other problems (e.g., regression problems) y can sometimes take values intermediate between 0 and 1. Show that the cross-entropy is still minimized when σ(z)=y for all training inputs. When this is the case the cross-entropy has the value:

<p align="center">
  <img src="http://latex2png.com/pngs/458e94840b009ae8a46d5849344ffb73.png" width="400"/><br>
  <b><a name="64">(64)</a></b>
</p>

The quantity −[ylny+(1−y)ln(1−y)] is sometimes known as the [binary entropy](http://en.wikipedia.org/wiki/Binary_entropy_function).

## Problems 

* **Many-layer multi-neuron networks** In the notation introduced in the [last chapter](http://neuralnetworksanddeeplearning.com/chap2.html), show that for the quadratic cost the partial derivative with respect to weights in the output layer is

<p align="center">
  <img src="http://latex2png.com/pngs/d79a7f53bc253175f7a189bede47240c.png" width="400"/><br>
  <b><a name="65">(65)</a></b>
</p>

The term σ′(z<sub>j</sub><sup>L</sup>) causes a learning slowdown whenever an output neuron saturates on the wrong value. Show that for the cross-entropy cost the output error δ<sup>L</sup> for a single training example x is given by


<p align="center">
  <img src="http://latex2png.com/pngs/e3aecd9e0cfe75f1fc8372b93f4ed4a1.png" width="400"/><br>
  <b><a name="66">(66)</a></b>
</p>

Use this expression to show that the partial derivative with respect to the weights in the output layer is given by

<p align="center">
  <img src="http://latex2png.com/pngs/3367f413a20b9fbcbd6dda6e61ff6876.png" width="400"/><br>
  <b><a name="67">(67)</a></b>
</p>

The σ′(z<sub>j</sub><sup>L</sup>) term has vanished, and so the cross-entropy avoids the problem of learning slowdown, not just when used with a single neuron, as we saw earlier, but also in many-layer multi-neuron networks. A simple variation on this analysis holds also for the biases. If this is not obvious to you, then you should work through that analysis as well.

* **Using the quadratic cost when we have linear neurons in the output layer** Suppose that we have a many-layer multi-neuron network. Suppose all the neurons in the final layer are linear neurons, meaning that the sigmoid activation function is not applied, and the outputs are simply a<sub>j</sub><sup>j</sup>=z<sub>j</sub><sup>L</sup>. Show that if we use the quadratic cost function then the output error δL for a single training example x is given by

<p align="center">
  <img src="http://latex2png.com/pngs/e3aecd9e0cfe75f1fc8372b93f4ed4a1.png" width="400"/><br>
  <b><a name="68">(68)</a></b>
</p>

Similarly to the previous problem, use this expression to show that the partial derivatives with respect to the weights and biases in the output layer are given by

<p align="center">
  <img src="http://latex2png.com/pngs/b60c991861408e5ddd7449ea62f042b1.png" width="400"/><br>
  <b><a name="69">(69)</a></b>
</p>

<p align="center">
  <img src="http://latex2png.com/pngs/a86ea75468101e228ab8d5ba2f42500f.png" width="400"/><br>
  <b><a name="70">(70)</a></b>
</p>

This shows that if the output neurons are linear neurons then the quadratic cost will not give rise to any problems with a learning slowdown. In this case the quadratic cost is, in fact, an appropriate cost function to use.

## Using the cross-entropy to classify MNIST digits

The cross-entropy is easy to implement as part of a program which learns using gradient descent and backpropagation. We'll do that [later in the chapter](http://neuralnetworksanddeeplearning.com/chap3.html#handwriting_recognition_revisited_the_code), developing an improved version of our [earlier program](http://neuralnetworksanddeeplearning.com/chap1.html#implementing_our_network_to_classify_digits) for classifying the MNIST handwritten digits, `network.py`. The new program is called `network2.py`, and incorporates not just the cross-entropy, but also several other techniques developed in this chapter. For now, let's look at how well our new program classifies MNIST digits. As was the case in Chapter 1, we'll use a network with 30 hidden neurons, and we'll use a mini-batch size of 10. We set the learning rate to η=0.5 and we train for 30 epochs. The interface to `network2.py` is slightly different than `network.py`, but it should still be clear what is going on. You can, by the way, get documentation about `network2.py`'s interface by using commands such as `help(network2.Network.SGD)` in a Python shell.

