# Other techniques
Each technique developed in this chapter is valuable to know in its own right, but that's not the only reason I've explained them. The larger point is to familiarize you with some of the problems which can occur in neural networks, and with a style of analysis which can help overcome those problems. In a sense, we've been learning how to think about neural nets. Over the remainder of this chapter I briefly sketch a handful of other techniques. These sketches are less in-depth than the earlier discussions, but should convey some feeling for the diversity of techniques available for use in neural networks.

## Variations on stochastic gradient descent

Stochastic gradient descent by backpropagation has served us well in attacking the MNIST digit classification problem. However, there are many other approaches to optimizing the cost function, and sometimes those other approaches offer performance superior to mini-batch stochastic gradient descent. In this section I sketch two such approaches, the Hessian and momentum techniques.

### Hessian technique

To begin our discussion it helps to put neural networks aside for a bit. Instead, we're just going to consider the abstract problem of minimizing a cost function C which is a function of many variables, w=w<sub>1</sub>,w<sub>2</sub>,…, so C=C(w). By Taylor's theorem, the cost function can be approximated near a point w by
<p align="center">
  <img src="https://user-images.githubusercontent.com/48807586/154579987-36c56cff-cebc-4596-85a8-a914aad76ec2.png" width="350"/><b>(103)</b>
</p>

We can rewrite this more compactly as

<p align="center">
  <img src="https://user-images.githubusercontent.com/48807586/154580225-ea366786-dcb1-4ca1-94df-4519f93ba7d0.png" width="350"/><b>(104)</b>
</p>

where ∇C is the usual gradient vector, and H is a matrix known as the _Hessian matrix_, whose j<sub>k</sub>th entry is ∂<sup>2</sup>C/∂w<sub>j</sub>∂w<sub>k</sub>. Suppose we approximate C by discarding the higher-order terms represented by … above,

<p align="center">
  <img src="https://user-images.githubusercontent.com/48807586/154580282-a17ef874-1d2b-4d37-99a8-9e6c3da51f76.png" width="350"/><b>(105)</b>
</p>

Using calculus we can show that the expression on the right-hand side can be minimized (strictly speaking, for this to be a minimum, and not merely an extremum, we need to assume that the Hessian matrix is positive definite. Intuitively, this means that the function C looks like a valley locally, not a mountain or a saddle) by choosing

<p align="center">
  <img src="https://user-images.githubusercontent.com/48807586/154580321-5ed9d45a-4977-41b3-a09e-980f334b51ea.png" width="150"/><b>(105)</b>
</p>

Provided (105) is a good approximate expression for the cost function, then we'd expect that moving from the point w to w+Δw=w−H<sup>−1</sup>∇C should significantly decrease the cost function. That suggests a possible algorithm for minimizing the cost:

- Choose a starting point, w.
- Update w to a new point w<sup>′</sup>=w−H<sup>−1</sup>∇C, where the Hessian H and ∇C are computed at w.
- Update w<sup>′</sup> to a new point w<sup>′′</sup>=w<sup>′</sup>−H<sup>′</sup>−1∇<sup>′</sup>C, where the Hessian H<sup>′</sup> and ∇<sup>′</sup>C are computed at w<sup>′</sup>.

In practice, (105) is only an approximation, and it's better to take smaller steps. We do this by repeatedly changing w by an amount Δw=−ηH<sup>−1</sup>∇C, where _η_ is known as the learning rate.

This approach to minimizing a cost function is known as the _Hessian technique_ or _Hessian optimization_. There are theoretical and empirical results showing that Hessian methods converge on a minimum in fewer steps than standard gradient descent. In particular, by incorporating information about second-order changes in the cost function it's possible for the Hessian approach to avoid many pathologies that can occur in gradient descent. Furthermore, there are versions of the backpropagation algorithm which can be used to compute the Hessian.

If Hessian optimization is so great, why aren't we using it in our neural networks? Unfortunately, while it has many desirable properties, it has one very undesirable property: it's very difficult to apply in practice. Part of the problem is the sheer size of the Hessian matrix. Suppose you have a neural network with 107 weights and biases. Then the corresponding Hessian matrix will contain 10<sup>7</sup>×10<sup>7</sup>=10<sup>14</sup> entries. That's a lot of entries! And that makes computing H<sup>−1</sup>∇C extremely difficult in practice. However, that doesn't mean that it's not useful to understand. In fact, there are many variations on gradient descent which are inspired by Hessian optimization, but which avoid the problem with overly-large matrices. Let's take a look at one such technique, momentum-based gradient descent.

### Momentum-based gradient descent

Intuitively, the advantage Hessian optimization has is that it incorporates not just information about the gradient, but also information about how the gradient is changing. Momentum-based gradient descent is based on a similar intuition, but avoids large matrices of second derivatives. To understand the momentum technique, think back to our original picture of gradient descent, in which we considered a ball rolling down into a valley. At the time, we observed that gradient descent is, despite its name, only loosely similar to a ball falling to the bottom of a valley. The momentum technique modifies gradient descent in two ways that make it more similar to the physical picture. First, it introduces a notion of "velocity" for the parameters we're trying to optimize. The gradient acts to change the velocity, not (directly) the "position", in much the same way as physical forces change the velocity, and only indirectly affect position. Second, the momentum method introduces a kind of friction term, which tends to gradually reduce the velocity.

Let's give a more precise mathematical description. We introduce velocity variables v=v<sub>1</sub>,v<sub>2</sub>,…, one for each corresponding w<sub>j</sub> variable (in a neural net the w<sub>j</sub> variables would, of course, include all weights and biases). Then we replace the gradient descent update rule w→w′=w−η∇C by

<p align="center">
  <img src="https://user-images.githubusercontent.com/48807586/154580476-c9bfd588-af0b-41bf-bdfb-0483a50a36f6.png" width="150"/><b>(107)</b>
</p>
<p align="center">
  <img src="https://user-images.githubusercontent.com/48807586/154580491-4cabcacc-d063-493b-bbb3-e85e69c4bd90.png" width="150"/><b>(108)</b>
</p>


In these equations, μ is a hyper-parameter which controls the amount of damping or friction in the system. To understand the meaning of the equations it's helpful to first consider the case where μ=1, which corresponds to no friction. When that's the case, inspection of the equations shows that the "force" ∇C is now modifying the velocity, v, and the velocity is controlling the rate of change of w. Intuitively, we build up the velocity by repeatedly adding gradient terms to it. That means that if the gradient is in (roughly) the same direction through several rounds of learning, we can build up quite a bit of steam moving in that direction. Think, for example, of what happens if we're moving straight down a slope:

<p align="center">
  <img src="https://user-images.githubusercontent.com/48807586/154580581-a7990d1d-9361-4b63-bf21-9b28631581aa.png" width="400"/>
</p>

With each step the velocity gets larger down the slope, so we move more and more quickly to the bottom of the valley. This can enable the momentum technique to work much faster than standard gradient descent. Of course, a problem is that once we reach the bottom of the valley we will overshoot. Or, if the gradient should change rapidly, then we could find ourselves moving in the wrong direction. That's the reason for the μ hyper-parameter in (107). I said earlier that μ controls the amount of friction in the system; to be a little more precise, you should think of 1−μ as the amount of friction in the system. When μ=1, as we've seen, there is no friction, and the velocity is completely driven by the gradient ∇C. By contrast, when μ=0 there's a lot of friction, the velocity can't build up, and Equations (107) and (108) reduce to the usual equation for gradient descent, w→w′=w−η∇C. In practice, using a value of μ intermediate between 0 and 1 can give us much of the benefit of being able to build up speed, but without causing overshooting. We can choose such a value for μ using the held-out validation data, in much the same way as we select η and λ.

I've avoided naming the hyper-parameter μ up to now. The reason is that the standard name for μ is badly chosen: it's called the _momentum co-efficient_. This is potentially confusing, since μ is not at all the same as the notion of momentum from physics. Rather, it is much more closely related to friction. However, the term momentum co-efficient is widely used, so we will continue to use it.

A nice thing about the momentum technique is that it takes almost no work to modify an implementation of gradient descent to incorporate momentum. We can still use backpropagation to compute the gradients, just as before, and use ideas such as sampling stochastically chosen mini-batches. In this way, we can get some of the advantages of the Hessian technique, using information about how the gradient is changing. But it's done without the disadvantages, and with only minor modifications to our code. In practice, the momentum technique is commonly used, and often speeds up learning.

## Exercise

- What would go wrong if we used μ>1 in the momentum technique?
- What would go wrong if we used μ<0 in the momentum technique?
