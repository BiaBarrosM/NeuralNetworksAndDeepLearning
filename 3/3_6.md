# Other techniques

## Variations on stochastic gradient descent

## Exercise

## Problem

- Add momentum-based stochastic gradient descent to `network2.py`.

**Other approaches to minimizing the cost function:** Many other approaches to minimizing the cost function have been developed, and there isn't universal agreement on which is the best approach. As you go deeper into neural networks it's worth digging into the other techniques, understanding how they work, their strengths and weaknesses, and how to apply them in practice. A paper I mentioned earlier<sup>[*](#fn1)</sup> introduces and compares several of these techniques, including conjugate gradient descent and the BFGS method (see also the closely related limited-memory BFGS method, known as [L-BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS)). Another technique which has recently shown promising results<sup>[*](#fn2)</sup> is Nesterov's accelerated gradient technique, which improves on the momentum technique. However, for many problems, plain stochastic gradient descent works well, especially if momentum is used, and so we'll stick to stochastic gradient descent through the remainder of this book.

<a name="fn1">*</a>[Efficient BackProp](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf), by Yann LeCun, Léon Bottou, Genevieve Orr and Klaus-Robert Müller (1998).

<a name="fn2">*</a>See, for example, [On the importance of initialization and momentum in deep learning](http://www.cs.toronto.edu/~hinton/absps/momentum.pdf), by Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton (2012).

## Other models of artificial neuron

Up to now we've built our neural networks using sigmoid neurons. In principle, a network built from sigmoid neurons can compute any function. In practice, however, networks built using other model neurons sometimes outperform sigmoid networks. Depending on the application, networks based on such alternate models may learn faster, generalize better to test data, or perhaps do both. Let me mention a couple of alternate model neurons, to give you the flavor of some variations in common use.

Perhaps the simplest variation is the tanh (pronounced "tanch") neuron, which replaces the sigmoid function by the hyperbolic tangent function. The output of a tanh neuron with input x, weight vector w, and bias b is given by

<p align="center">
  <img src="http://latex2png.com/pngs/98ece92bb69a140309b8f86a42e24b2c.png" width="400"/>
  <b><a name="109">(109)</a></b>
</p>

where tanh is, of course, the hyperbolic tangent function. It turns out that this is very closely related to the sigmoid neuron. To see this, recall that the tanh function is defined by

<p align="center">
  <img src="http://latex2png.com/pngs/373f2fb81bcf05be3df8967c8ae2f650.png" width="400"/>
  <b><a name="110">(110)</a></b>
</p>

With a little algebra it can easily be verified that

<p align="center">
  <img src="http://latex2png.com/pngs/e61cf409ee4da458e5507437a0234597.png" width="400"/>
  <b><a name="111">(111)</a></b>
</p>

that is, tanh is just a rescaled version of the sigmoid function. We can also see graphically that the tanh function has the same shape as the sigmoid function,

<p align="center">
  <img src="https://user-images.githubusercontent.com/77112891/157322000-cf9a202d-b946-4146-ae8e-1171c675034b.png" width="400"/>
</p>

One difference between tanh neurons and sigmoid neurons is that the output from tanh neurons ranges from -1 to 1, not 0 to 1. This means that if you're going to build a network based on tanh neurons you may need to normalize your outputs (and, depending on the details of the application, possibly your inputs) a little differently than in sigmoid networks.

Similar to sigmoid neurons, a network of tanh neurons can, in principle, compute any function<sup>[*](#fn3)</sup> mapping inputs to the range -1 to 1. Furthermore, ideas such as backpropagation and stochastic gradient descent are as easily applied to a network of tanh neurons as to a network of sigmoid neurons.

<a name="fn3">*</a>There are some technical caveats to this statement for both tanh and sigmoid neurons, as well as for the rectified linear neurons discussed below. However, informally it's usually fine to think of neural networks as being able to approximate any function to arbitrary accuracy.

